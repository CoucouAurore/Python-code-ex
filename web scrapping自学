import requests
from bs4 import BeautifulSoup
import selenium

print("ok")


class UrlManager():
    """
    url管理器
    """

    def __init__(self):初始化函数
        self.new_urls=set()定义新的待爬取的url
        self.old_urls=set()已经爬取的url

    def add_new_url(self,url):新增url
        if url is None or len(url)==0如果这个变量为none或者长度为0
            return
        if url in self.new_urls or url in self.old_urls: 判断这个url是否已经在容器里面了
        return
        self.new_urls.add(url)


    def add_new_url(self,urls):批量添加urls
        if urls is None or len(urls)==0:
            return

        for url in urls:
            self.add_new_url(url)

    def get_url(self):取出url
        if self.has_new_url():
            url=self.new_urls.pop()
            self.old_urls.add(url)
            return url
        else:
            return None

    def has_new_url(self):是否有带爬取的url
        return len(self.new_urls)>0


创建BeautifulSoup对象
From bs4 import BeautifulSoup

#根据HTML网页字符串创建BeautifulSoup对象
soup=BeautifulSoup(
                            html_doc, #HTML文档字符串
                            ‘html.parser’, #HTML解析器
                            from_encoding=‘utf8’ #HTML文档的编码
                            )

搜索节点（find_all,find)
#方法：find_all(name名称, attrs属性, string文本)

#查找所有标签为a的节点
soup.find_all(‘a’)

#查找所有标签为a，链接符合/view/123.html形式的节点
soup.find_all(‘a’, href=‘/view/123.html’)

#查找所有标签为div,class为abc,文字为Python的节点
soup.find_all(‘div’, class_=‘abc’, string=‘Python’)


访问节点信息
#得到节点： < a href=‘1.html’>Python</a>
#获取查找到的节点的标签名称
node.name

#获取查找到的a节点的href属性
node[‘href’]
#获取查找到的a节点的链接文字
node.get_text()

