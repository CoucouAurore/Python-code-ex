Python code for web scraping using the BeautifulSoup library and the requests library. 
In this example, we'll scrape the titles and links of articles from a hypothetical website.

First, you'll need to install the necessary libraries if you haven't already:
pip install requests beautifulsoup4


Now, let's write the scraping code:
import requests
from bs4 import BeautifulSoup

# URL of the website you want to scrape
url = 'https://example.com'

# Send an HTTP GET request to the URL
response = requests.get(url)

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # Parse the HTML content of the page using BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Find and extract the elements you're interested in
    # In this example, we're scraping article titles and links
    article_elements = soup.find_all('a', class_='article-link')
    
    for article in article_elements:
        title = article.text
        link = article['href']
        
        # Print the title and link of each article
        print(f'Title: {title}')
        print(f'Link: {link}')
else:
    print('Failed to retrieve the web page.')

# Be a good web citizen and close the HTTP session
response.close()




In this code:

We import the required libraries, requests for making HTTP requests and BeautifulSoup for parsing HTML.
We specify the URL of the website we want to scrape.
We send an HTTP GET request to the URL and check if the request was successful (status code 200).
If the request was successful, we use BeautifulSoup to parse the HTML content of the page.
We use find_all to locate all the elements with the specified tag and class (in this case, 'a' elements with the class 'article-link').
We loop through the found elements and extract the text (article title) and the 'href' attribute (article link) for each article.
Finally, we print the title and link of each article.
Make sure to adapt the code to the structure of the website you want to scrape and consider the website's terms of service and robots.txt file to ensure you're scraping ethically and legally.
